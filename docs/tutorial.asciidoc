= Debezium Tutorial
:awestruct-layout: doc
:linkattrs:
:icons: font
:debezium-version: 0.3.0
:debezium-docker-label: 0.3
:debezium-kafka-version: 0.10.0.0

This tutorial walks you through running Debezium {debezium-version} for change data capture (CDC). You will use Docker (1.9 or later) to start the Debezium services, run a MySQL database server with a simple example database, use Debezium to monitor the database, and see the resulting event streams respond as the data in the database changes.

== What is Debezium?

Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond immediately to each row-level change in the databases. Debezium is built on top of http://kafka.apache.org[Apache Kafka] and provides http://kafka.apache.org/documentation.html#connect[Kafka Connect] compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, from where your application consumes them. This makes it possible for your application to easily consume all of the events correctly and completely. Even if your application stops (or crashes), upon restart it will start consume the events where it left off so it misses nothing.

Debezium {debezium-version} includes support for monitoring MySQL database servers with its link:/docs/connectors/mysql[MySQL connector], and this is what we'll use in this demonstration. Support for other DBMSes will be added in future releases.

== Running Debezium with Docker

Running Debezium involves three major services: http://zookeeper.apache.org[Zookeeper], Kafka, and Debezium's connector service. This tutorial walks you through starting a single instance of these services using http://docker.com[Docker] and https://hub.docker.com/u/debezium/[Debezium's Docker images]. Production environments, on the other hand, require running multiple instances of each service to provide the performance, reliability, replication, and fault tolerance. This can be done with a platform like https://www.openshift.com[OpenShift] and http://kubernetes.io[Kubernetes] that manages multiple Docker containers running on multiple hosts and machines, but often you'll want to link:/docs/install[install on dedicated hardware].

== Starting Docker

Make sure that Docker is https://docs.docker.com/engine/installation/[installed and running]. If you're running Linux, you can either run the Docker daemon or configure it to run automatically on startup. In this case, the _Docker host_ is your local machine.

If you're using Windows or OS X, you have to run the Docker daeomon in a virtual machine. See our link:/docs/docker[Docker documentation] for detailed instructions on installing, starting, and using Docker Machine. Every time you start a new terminal, you need to configure the Docker Machine environment so any Docker commands you run on your host computer can communicate with the Docker daemon running in the virtual machine:

[source,bash,indent=0]
----
    $ eval $(docker-machine env)
----

== Starting simple with Debezium

For simple evaluation and experimentation, this tutorial will walk you through starting a single instance of each service in a separate container on your local machine. Zookeeper and Kafka both store data locally inside the container, and normal usage requires mounting directories on the host machines as volumes so that when the containers stop the persisted data will remain. We're skipping that in this tutorial, although the documentation for our https://hub.docker.com/r/debezium/[Docker images] describes how to do that. This means that when a container is removed, all persisted data will be lost. That's actually ideal for our experiment, since nothing will be left on your computer when we're finished, and you can run this experiment many times without having to clean anything up in between.

Running multiple services locally can be confusing, so we're going to use a separate terminal to run each container in the foreground. This way all of the output of a container will be displayed in the terminal used to run it.

[NOTE]
====
This is not the only way to run Docker containers. Rather than running a container in the foreground (with `--it`), Docker lets you run a container in _detached_ mode (with `-d`), where the container is started and the Docker command returns immediately. Detached mode containers don't display their output in the terminal, though you can always see the output by using `docker logs --follow --name <container-name>`. This is one reason we name each of the containers we run. See the Docker documentation for more detail.
====

[[start-zookeeper]]
=== Start Zookeeper

Of all the different services/processes that make up Debezium, the first one to start is Zookeeper. Start a new terminal with the link:/docs/docker[Docker environment], and then start a container with Zookeeper by running:

[source,bash,indent=0]
----
    $ docker run -it --rm --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 debezium/zookeeper:0.3
----

This runs a new container using version {debezium-docker-label} of the `debezium/zookeeper` image, and assigns the name `zookeeper` to this container. The `-it` flag makes the container interactive, meaning it attaches the terminal's standard input and output to the container so that you can see what is going on in the container. The `--rm` flag instructs Docker to remove the container when it is stopped. The three `-p` options map three of the container's ports (e.g., 2181, 2888, and 3888) to the same ports on the Docker host so that other containers (and software outside the container) can talk with Zookeeper.

You should see in your terminal the typical output of Zookeeper:

[listing,indent=0,options="nowrap"]
----
Starting up in standalone mode
ZooKeeper JMX enabled by default
Using config: /zookeeper/conf/zoo.cfg
2016-08-16 00:54:57,417 - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /zookeeper/conf/zoo.cfg
2016-08-16 00:54:57,421 - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
2016-08-16 00:54:57,421 - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
2016-08-16 00:54:57,422 - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode
2016-08-16 00:54:57,425 - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
2016-08-16 00:54:57,437 - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
2016-08-16 00:54:57,446 - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /zookeeper/conf/zoo.cfg
2016-08-16 00:54:57,447 - INFO  [main:ZooKeeperServerMain@95] - Starting server
2016-08-16 00:54:57,465 - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.8--1, built on 02/06/2016 03:18 GMT
2016-08-16 00:54:57,466 - INFO  [main:Environment@100] - Server environment:host.name=62de4b8ae43b
2016-08-16 00:54:57,466 - INFO  [main:Environment@100] - Server environment:java.version=1.8.0_92
2016-08-16 00:54:57,467 - INFO  [main:Environment@100] - Server environment:java.vendor=Azul Systems, Inc.
2016-08-16 00:54:57,467 - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
2016-08-16 00:54:57,468 - INFO  [main:Environment@100] - Server environment:java.class.path=/zookeeper/bin/../build/classes:/zookeeper/bin/../build/lib/*.jar:/zookeeper/bin/../lib/slf4j-log4j12-1.6.1.jar:/zookeeper/bin/../lib/slf4j-api-1.6.1.jar:/zookeeper/bin/../lib/netty-3.7.0.Final.jar:/zookeeper/bin/../lib/log4j-1.2.16.jar:/zookeeper/bin/../lib/jline-0.9.94.jar:/zookeeper/bin/../zookeeper-3.4.8.jar:/zookeeper/bin/../src/java/lib/*.jar:/zookeeper/conf:
2016-08-16 00:54:57,468 - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2016-08-16 00:54:57,468 - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
2016-08-16 00:54:57,471 - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
2016-08-16 00:54:57,474 - INFO  [main:Environment@100] - Server environment:os.name=Linux
2016-08-16 00:54:57,474 - INFO  [main:Environment@100] - Server environment:os.arch=amd64
2016-08-16 00:54:57,474 - INFO  [main:Environment@100] - Server environment:os.version=4.4.12-boot2docker
2016-08-16 00:54:57,475 - INFO  [main:Environment@100] - Server environment:user.name=zookeeper
2016-08-16 00:54:57,475 - INFO  [main:Environment@100] - Server environment:user.home=/zookeeper
2016-08-16 00:54:57,475 - INFO  [main:Environment@100] - Server environment:user.dir=/zookeeper
2016-08-16 00:54:57,480 - INFO  [main:ZooKeeperServer@787] - tickTime set to 2000
2016-08-16 00:54:57,481 - INFO  [main:ZooKeeperServer@796] - minSessionTimeout set to -1
2016-08-16 00:54:57,482 - INFO  [main:ZooKeeperServer@805] - maxSessionTimeout set to -1
2016-08-16 00:54:57,495 - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
----

The last line is important and reports that Zookeeper is ready and listening on port 2181. The terminal will continue to show additional output as Zookeeper generates it.

[[start-kafka]]
=== Start Kafka

Open a new terminal and configure it with the link:/docs/docker[Docker environment]. Then, start Kafka in a new container by running:

[source,bash,indent=0]
----
    $ docker run -it --rm --name kafka -p 9092:9092 --link zookeeper:zookeeper debezium/kafka:0.3
----

[NOTE]
====
In this tutorial we're always connecting to Kafka from within a Docker container, and they'll always be able to see and communicate with the `kafka` container as long as we link to the `kafka` container. If we wanted to connect to Kafka from _outside_ of a Docker container, then we'd want Kafka to _advertise_ it's address via the Docker host, which we could do by adding `-e ADVERTISED_HOST_NAME=address` followed by the IP address or resolvable hostname of the Docker host. When using Docker Machine that's just `$(docker-machine ip)`, or on Linux it's the IP address of the host computer (not `localhost`).
====

This runs a new container using version {debezium-docker-label} of the `debezium/kafka` image, and assigns the name `kafka` to this container. The `-it` flag makes the container interactive, meaning it attaches the terminal's standard input and output to the container so that you can see what is going on in the container. The `--rm` flag instructs Docker to remove the container when it is stopped. The command maps port 9092 in the container to the same port on the Docker host so that software outside of the container can talk with Kafka. Finally, the command uses the `--link zookeeper:zookeeper` argument to tell the container that it can find Zookeeper in the container named `zookeeper` running on the same Docker host.

You should see in your terminal the typical output of Kafka, ending with:

[listing,indent=0,options="nowrap"]
----
...
2016-08-16 00:55:06,254 - INFO  [main-EventThread:ZkClient@712] - zookeeper state changed (SyncConnected)
2016-08-16 00:55:06,380 - INFO  [main:Logging$class@68] - Loading logs.
2016-08-16 00:55:06,385 - INFO  [main:Logging$class@68] - Logs loading complete.
2016-08-16 00:55:06,427 - INFO  [main:Logging$class@68] - Starting log cleanup with a period of 300000 ms.
2016-08-16 00:55:06,429 - INFO  [main:Logging$class@68] - Starting log flusher with a default period of 9223372036854775807 ms.
2016-08-16 00:55:06,437 - WARN  [main:Logging$class@83] - No meta.properties file under dir /kafka/data/1/meta.properties
2016-08-16 00:55:06,492 - INFO  [main:Logging$class@68] - Awaiting socket connections on 172.17.0.3:9092.
2016-08-16 00:55:06,495 - INFO  [main:Logging$class@68] - [Socket Server on Broker 1], Started 1 acceptor threads
2016-08-16 00:55:06,547 - INFO  [ExpirationReaper-1:Logging$class@68] - [ExpirationReaper-1], Starting 
2016-08-16 00:55:06,548 - INFO  [ExpirationReaper-1:Logging$class@68] - [ExpirationReaper-1], Starting 
2016-08-16 00:55:06,602 - INFO  [main:Logging$class@68] - Creating /controller (is it secure? false)
2016-08-16 00:55:06,610 - INFO  [main:Logging$class@68] - Result of znode creation is: OK
2016-08-16 00:55:06,611 - INFO  [main:Logging$class@68] - 1 successfully elected as leader
2016-08-16 00:55:06,705 - INFO  [ExpirationReaper-1:Logging$class@68] - [ExpirationReaper-1], Starting 
2016-08-16 00:55:06,711 - INFO  [ExpirationReaper-1:Logging$class@68] - [ExpirationReaper-1], Starting 
2016-08-16 00:55:06,735 - INFO  [group-metadata-manager-0:Logging$class@68] - [Group Metadata Manager on Broker 1]: Removed 0 expired offsets in 3 milliseconds.
2016-08-16 00:55:06,737 - INFO  [main:Logging$class@68] - [GroupCoordinator 1]: Starting up.
2016-08-16 00:55:06,750 - INFO  [main:Logging$class@68] - [GroupCoordinator 1]: Startup complete.
2016-08-16 00:55:06,795 - INFO  [ThrottledRequestReaper-Produce:Logging$class@68] - [ThrottledRequestReaper-Produce], Starting 
2016-08-16 00:55:06,798 - INFO  [ThrottledRequestReaper-Fetch:Logging$class@68] - [ThrottledRequestReaper-Fetch], Starting 
2016-08-16 00:55:06,827 - INFO  [main:Logging$class@68] - Will not load MX4J, mx4j-tools.jar is not in the classpath
2016-08-16 00:55:06,871 - INFO  [main:Logging$class@68] - Creating /brokers/ids/1 (is it secure? false)
2016-08-16 00:55:06,888 - INFO  [main:Logging$class@68] - Result of znode creation is: OK
2016-08-16 00:55:06,889 - INFO  [main:Logging$class@68] - Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT -> EndPoint(172.17.0.3,9092,PLAINTEXT)
2016-08-16 00:55:06,892 - WARN  [main:Logging$class@83] - No meta.properties file under dir /kafka/data/1/meta.properties
2016-08-16 00:55:06,920 - INFO  [ZkClient-EventThread-14-172.17.0.2:2181:Logging$class@68] - New leader is 1
2016-08-16 00:55:06,961 - INFO  [main:AppInfoParser$AppInfo@83] - Kafka version : 0.10.0.1
2016-08-16 00:55:06,962 - INFO  [main:AppInfoParser$AppInfo@84] - Kafka commitId : a7a17cdec9eaa6c5
2016-08-16 00:55:06,963 - INFO  [main:Logging$class@68] - [Kafka Server 1], started
----

The last line shown above reports that the Kafka broker has successfully started and is ready for client connections. The terminal will continue to show additional output as Kafka generates it.

[TIP]
====
Debezium {debezium-version} requires Kafka Connect {debezium-kafka-version}, and in this tutorial we also use version {debezium-kafka-version} of the Kafka broker. Check the http://kafka.apache.org/documentation.html[Kafka documentation] about compatibility between different versions of Kafka Connect and the Kafka broker.
====

[[start-kafka-connect]]
=== Start Kafka Connect

Open a new terminal and configure it with the link:/docs/docker[Docker environment]. In that terminal, start the Kafka Connect service in a new container by running:

[source,bash,indent=0]
----
    $ docker run -it --rm --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my_connect_offsets --link zookeeper:zookeeper --link kafka:kafka debezium/connect:0.3
----

This runs a new container named `connect` using version {debezium-docker-label} of the `debezium/connect` image. The `-it` flag makes the container interactive, meaning it attaches the terminal's standard input and output to the container so that you can see what is going on in the container. The `--rm` flag instructs Docker to remove the container when it is stopped. The command maps port 8083 in the container to the same port on the Docker host so that software outside of the container can use Kafka Connect's REST API to set up and manage new connector instances. The command uses the `--link zookeeper:zookeeper` and `--link kafka:kafka` argument to tell the container that it can find Zookeeper and Kafka in the container named `zookeeper` and `kafka`, respectively, running on the same Docker host. And finally, it also uses the `-e` option three times to set the `GROUP_ID`, `CONFIG_STORAGE_TOPIC`, and `OFFSET_STORAGE_TOPIC` environment variables, which are all required by this Debezium image (though you can use different values as desired).

You should see in your terminal the typical output of Kafka, ending with:

[listing,indent=0,options="nowrap"]
----
...
2016-08-16 01:22:42,083 INFO   ||  Kafka version : 0.10.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2016-08-16 01:22:42,083 INFO   ||  Kafka commitId : a7a17cdec9eaa6c5   [org.apache.kafka.common.utils.AppInfoParser]
2016-08-16 01:22:42,552 INFO   ||  Discovered coordinator 172.17.0.3:9092 (id: 2147483646 rack: null) for group 1.   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 01:22:42,565 INFO   ||  Finished reading KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2016-08-16 01:22:42,566 INFO   ||  Started KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2016-08-16 01:22:42,566 INFO   ||  Started KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2016-08-16 01:22:42,573 INFO   ||  Herder started   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2016-08-16 01:22:42,681 INFO   ||  Discovered coordinator 172.17.0.3:9092 (id: 2147483646 rack: null) for group 1.   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 01:22:42,682 INFO   ||  (Re-)joining group 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 01:22:42,883 INFO   ||  Successfully joined group 1 with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 01:22:42,883 INFO   ||  Joined group and got assignment: Assignment{error=0, leader='connect-1-f0fc4cad-6ff3-47ef-83dc-d1acf3c4ed82', leaderUrl='http://172.17.0.4:9092/', offset=-1, connectorIds=[], taskIds=[]}   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2016-08-16 01:22:42,884 INFO   ||  Starting connectors and tasks using config offset -1   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2016-08-16 01:22:42,885 INFO   ||  Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2016-08-16 01:22:45,920 INFO   ||  Reflections took 3270 ms to scan 69 urls, producing 3313 keys and 23410 values    [org.reflections.Reflections]
----

The last few line shown above reports that the service has started and is ready for connections. The terminal will continue to show additional output as the Kafka Connect service generates it.

[[kafka-connect-api]]
==== Using the Kafka Connect REST API

The Kafka Connect service exposes a RESTful API to manage the set of connectors, so let's use that API using the `curl` command line tool. Because we mapped port 8083 in the `connect` container (where the Kafka Connect service is running) to port 8083 on the Docker host, we can communicate to the service by sending the request to port 8083 on the Docker host, which then forwards the request to the Kakfa Connect service.

Open a new terminal and configure it with the link:/docs/docker[Docker environment], and in that terminal run the following command to check the status of the Kafka Connect service:

[source,bash,indent=0]
----
    $ curl -H "Accept:application/json" $(docker-machine ip):8083/
----

The Kafka Connect service should return a JSON response message similar to the following:

[source,json,indent=0]
----
    {"version":"0.10.0.1","commit":"a7a17cdec9eaa6c5"}
----

This shows that we're running Kafka Connect version 0.10.0.1. Next, check the list of connectors:

[source,bash,indent=0]
----
    $ curl -H "Accept:application/json" $(docker-machine ip):8083/connectors/
----

which should return the following:

[source,json,indent=0]
----
    []
----

This confirms that the Kafka Connect service is running, that we can talk with it, and that it currently has no connectors.


[[start-mysql]]
=== Start a MySQL database

At this point, we've started Zookeeper, Kafka, and Kafka Connect, but we've not yet configured Kafka Connect to run any connectors. In other words, the basic Debezium services are running but they're not yet watching any databases. Before we can set up connectors, we first need a relational database to monitor.

Open a new terminal and configure it with the link:/docs/docker[Docker environment]. In that terminal, start a new container that runs a MySQL database server preconfigured with an `inventory` database:

[source,bash,indent=0]
----
    $ docker run -it --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.3
----

This runs a new container using version {debezium-docker-label} of the `debezium/example-mysql` image, which is https://github.com/debezium/docker-images/blob/master/examples/mysql/0.1/Dockerfile[based on] the https://hub.docker.com/r/_/mysql/[mysql:5.7] image, defines and populate a sample "inventory" database, and creates a `debezium` user with password `dbz` that has the minimum privileges required by Debezium's MySQL connector. The command assigns the name `mysql` to the container so that it can be easily referenced later. The `-it` flag makes the container interactive, meaning it attaches the terminal's standard input and output to the container so that you can see what is going on in the container. The `--rm` flag instructs Docker to remove the container when it is stopped. The command maps port 3036 (the default MySQL port) in the container to the same port on the Docker host so that software outside of the container can connect to the database server. And finally, it also uses the `-e` option three times to set the `MYSQL_ROOT_PASSWORD`, `MYSQL_USER`, and `MYSQL_PASSWORD` environment variables to specific values.

You should see in your terminal something like the following:

[listing,indent=0,options="nowrap"]
----
...
Initializing database
Database initialized
MySQL init process in progress...
Warning: Unable to load '/usr/share/zoneinfo/Factory' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/posix/Factory' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/right/Factory' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
mysql: [Warning] Using a password on the command line interface can be insecure.
mysql: [Warning] Using a password on the command line interface can be insecure.

/usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/inventory.sql
mysql: [Warning] Using a password on the command line interface can be insecure.



MySQL init process done. Ready for start up.
----

Notice that the MySQL server starts and stops a few times as the configuration is modified. The `Ready for start up` line reports that the MySQL server is running and ready for use.

[[start-mysql-command-line]]
=== Start a MySQL command line client

Open a new terminal and configure it with the link:/docs/docker[Docker environment]. In that terminal, run the following to start a new container to run the MySQL command line client and connect it to the MySQL server running in the `mysql` container:

[source,bash,indent=0]
----
    $ docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'
----

Here we start the container using the https://hub.docker.com/r/_/mysql/[mysql:5.7] image, name the container `mysqlterm` and link it to the `mysql` container where the database server is running. The `--rm` option tells Docker to remove the container when it stops, and the rest of the command defines the shell command that the container should run. This shell command runs the MySQL command line client and specifies the correct options so that it can connect properly.

The container should output lines similar to the following:

[source,bash,indent=0]
----
    mysql: [Warning] Using a password on the command line interface can be insecure.
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 2
    
    Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.
    
    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.
    
    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
    
    mysql> 
----

Unlike the other containers, this container runs a process that produces a prompt. We'll use the prompt to interact with the database. First, switch to the "inventory" database:

[source,sql,indent=0]
----
    mysql> use inventory;
----

and then list the tables in the database:

[source,sql,indent=0]
----
    mysql> show tables;
----

which should then display:

[source,sql,indent=0]
----
    +---------------------+
    | Tables_in_inventory |
    +---------------------+
    | customers           |
    | orders              |
    | products            |
    | products_on_hand    |
    +---------------------+
    4 rows in set (0.00 sec)
----

Use the MySQL command line client to explore the database and view the pre-loaded data in the database. For example:

[source,sql,indent=0]
----
    mysql> SELECT * FROM customers;
----

[[monitor-mysql]]
=== Monitor the MySQL database

At this point we are running the Debezium services, a MySQL database server with a sample `inventory` database, and the MySQL command line client that is connected to our database. The next step is to register a connector that will begin monitoring the MySQL database server's binlog and generate change events for each row that has been (or will be) changed. Since this is a new connector, when it starts it will start reading from the beginning of the MySQL binlog, which records all of the transactions, including individual row changes and changes to the schemas. 

It is essential that the connector keep track of the schema changes, because each row change is recorded in the binlog in terms of the structure of its table _at the time the row was changed_. As our connector reads the binlog, the connector is actually replaying the history of the database and must keep track of the structure of each table to properly interpret the row changes. MySQL records in the binlog all DDL statements that change the database schema, so Debezium's MySQL connector parses and uses these DDL statements to maintain an in-memory model of the structure of each table. It also records these DDL statements in a separate Kafka topic so that the connector can recover the structure of the database that existed at any point in time, as defined by the statements in the binlog.

So before we start the connector, we need to create that Kafka topic where the connector can write out the database's schema history. We'll use the `debezium/kafka` image to start a container that runs the Kafka utility to create a `schema-changes.inventory` topic. 

Go back to your terminal where you ran the `curl` commands against the Kafka Connect service, and run the following to create the topic that our connector will use to record the DDL statements:

[source,bash,indent=0]
----
    $ docker run -it --rm --link zookeeper:zookeeper debezium/kafka:0.3 create-topic -r 1 dbhistory.inventory
----

The command runs a container using version {debezium-docker-label} of the `debezium/kafka` image, uses `--rm` to tell Docker to remove the container when it stops, and links to the Zookeeper container so that the utility can find the Kafka broker(s). The command runs the `create-topic` utility, which by default create a topic with one partition - exactly what we want so that total order of all DDL statements is maintained. The `-r 1` argument specifies the topic should have 1 replica.

[NOTE]
====
Normally we'd want 3 or more replicas so that we reduce the risk of losing data should brokers fail. But since we're just running a single broker in our tutorial, we can only specify 1 replia.
====

You'll see output similar to the following:

[source,indent=0]
----
...
Creating new topic dbhistory.inventory with 1 partition(s) and 1 replica(s)...
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic "dbhistory.inventory".
----

[TIP]
====
The container exits as soon as the request to create the topic completes, and because `--rm` is used Docker will remove the container, too.
====

Now we're ready to start our connector. Using the same terminal, we'll use `curl` to submit to our Kafka Connect service a JSON request message with information about our connector:

[source,bash,indent=0]
----
    $ curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" 192.168.99.100:8083/connectors/ -d '{ "name": "inventory-connector", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "192.168.99.100", "database.port": "3306", "database.user": "debezium", "database.password": "dbz", "database.server.id": "184054", "database.server.name": "dbserver1", "database.whitelist": "inventory", "database.history.kafka.bootstrap.servers": "kafka:9092", "database.history.kafka.topic": "dbhistory.inventory" } }'
----

[WARNING]
====
This command and several others use `192.168.99.100` as the IP address, which in my case is the IP address of the Docker host when using Docker Machine. If you're using Docker Machine, use `docker-machine ip` to get the IP address of your Docker host and use that address in the above command. If you're running Linux, get the IP address of your machine and update the `curl` command to use your IP address.
====

This command uses the Kafka Connect service's RESTful API to submit a `POST` request against `/connectors` resource with a JSON document that describes our new connector. Here's the same JSON message in a more readable format:

[source,json,indent=0]
----
    {
    	"name": "inventory-connector", 
    	"config": {
            "name": "inventory-connector",
            "connector.class": "io.debezium.connector.mysql.MySqlConnector",
            "tasks.max": "1",
            "database.hostname": "192.168.99.100",
            "database.port": "3306",
            "database.user": "debezium",
            "database.password": "dbz",
            "database.server.id": "184054",
            "database.server.name": "dbserver1",
            "database.whitelist": "inventory",
            "database.history.kafka.bootstrap.servers": "kafka:9092",
            "database.history.kafka.topic": "schema-changes.inventory",
        }
    }
----

The JSON message specifies the connector name as `inventory-connector`, and provides the detailed link:/docs/connectors/mysql#configuration[configuration properties for our MySQL connector]:

* Exactly one task should operate at any one time. Since the MySQL connect reads the MySQL server's binlog, and using a single connector task is the only way to ensure the proper order and that all events are handled properly.
* The database host and port are specified.
* The MySQL database we're running has a `replicator` user set up expressly for our purposes, so we specify that username and password here.
* A unique server ID and name are given. The server name is the logical identifier for the MySQL server or cluster of servers, and will be used as the prefix for all Kafka topics.
* The name of the initial binlog file is given. We start at the first file, but you can alternatively specify others.
* We only want to detect changes in the `inventory` database, so we use a whitelist.
* The connector should store the history of the database schemas in Kafka using the named broker (the same broker to which we're sending events) and topic name. Upon restart, the connector will recover the schemas of the database(s) that existed at the point in time in the binlog when the connector should begin reading.

This command should produce a response similar to the following (perhaps a bit more compact):

[source,http,indent=0]
----
HTTP/1.1 201 Created
Date: Tue, 16 Aug 2016 01:25:16 GMT
Location: http://192.168.99.100:8083/connectors/inventory-connector
Content-Type: application/json
Content-Length: 480
Server: Jetty(9.2.15.v20160210)

{
   "name":"inventory-connector",
   "config":{  
      "connector.class":"io.debezium.connector.mysql.MySqlConnector",
      "tasks.max":"1",
      "database.hostname":"192.168.99.100",
      "database.port":"3306",
      "database.user":"debezium",
      "database.password":"dbz",
      "database.server.id":"184054",
      "database.server.name":"dbserver1",
      "database.whitelist":"inventory",
      "database.history.kafka.bootstrap.servers":"kafka:9092",
      "database.history.kafka.topic":"dbhistory.inventory",
      "name":"inventory-connector"
   },
   "tasks":[]
}
----

This response describes the connector resource `/connectors/inventory-connector` that the service just created and includes the connector's configuration and information about the tasks. Since the connector was just created, the service hasn't yet finished starting tasks. 

We can even use the RESTful API to verify that our connector is included in the list of connectors:

[source,bash,indent=0]
----
    $ curl -H "Accept:application/json" 192.168.99.100:8083/connectors/
----

which should return the following:

[source,json,indent=0]
----
    ["inventory-connector"]
----

Recall that the Kafka Connect service uses connectors to start one or more tasks that do the work, and that it will automatically distribute the running tasks across the cluster of Kafka Connect services. Should any of the services stop or crash, those tasks will be redistributed to running services. We can see the tasks when we get the state of the connector:

[source,bash,indent=0]
----
    $ curl -i -X GET -H "Accept:application/json" 192.168.99.100:8083/connectors/inventory-connector
----

which returns:

[source,http,indent=0]
----
HTTP/1.1 200 OK
Date: Tue, 16 Aug 2016 01:13:45 GMT
Content-Type: application/json
Content-Length: 534
Server: Jetty(9.2.15.v20160210)

{
  "name": "inventory-connector",
  "config": {
    "name": "inventory-connector",
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    "database.hostname": "192.168.99.100",
    "database.port": "3306"
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "database.server.name": "mysql-server-1",
    "database.history.kafka.bootstrap.servers": "kafka:9092",
    "database.history.kafka.topic": "schema-changes.inventory",
    "database.whitelist": "inventory",
  },
  "tasks": [
    {
      "connector": "inventory-connector",
      "task": 0
    }
  ]
}
----

Here, we can see that the connector is running a single task (e.g., task 0) to do its work. The MySQL connector only supports a single task, since MySQL records all of its activities in one sequential binlog and so the MySQL connector needs only one reader to get a consistent and totally ordered view of all of those events. 

If we look at the output of our `connect` container, we should now see log statements that show the progress of the snapshot:

[listing,indent=0,options="nowrap"]
----
...
2016-08-16 01:25:19,908 INFO   MySQL|dbserver1|snapshot  Snapshot is using user 'debezium' with these MySQL grants:   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,910 INFO   MySQL|dbserver1|snapshot  	GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'debezium'@'%'   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,911 INFO   MySQL|dbserver1|snapshot  MySQL server variables related to change data capture:   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,915 INFO   MySQL|dbserver1|snapshot  	binlog_cache_size                             = 32768                                           [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,916 INFO   MySQL|dbserver1|snapshot  	binlog_checksum                               = CRC32                                           [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,916 INFO   MySQL|dbserver1|snapshot  	binlog_direct_non_transactional_updates       = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,917 INFO   MySQL|dbserver1|snapshot  	binlog_error_action                           = ABORT_SERVER                                    [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,917 INFO   MySQL|dbserver1|snapshot  	binlog_format                                 = ROW                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,918 INFO   MySQL|dbserver1|snapshot  	binlog_group_commit_sync_delay                = 0                                               [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,918 INFO   MySQL|dbserver1|snapshot  	binlog_group_commit_sync_no_delay_count       = 0                                               [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,919 INFO   MySQL|dbserver1|snapshot  	binlog_gtid_simple_recovery                   = ON                                              [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,920 INFO   MySQL|dbserver1|snapshot  	binlog_max_flush_queue_time                   = 0                                               [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,920 INFO   MySQL|dbserver1|snapshot  	binlog_order_commits                          = ON                                              [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,921 INFO   MySQL|dbserver1|snapshot  	binlog_row_image                              = FULL                                            [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,921 INFO   MySQL|dbserver1|snapshot  	binlog_rows_query_log_events                  = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,921 INFO   MySQL|dbserver1|snapshot  	binlog_stmt_cache_size                        = 32768                                           [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,922 INFO   MySQL|dbserver1|snapshot  	enforce_gtid_consistency                      = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,922 INFO   MySQL|dbserver1|snapshot  	gtid_executed_compression_period              = 1000                                            [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,923 INFO   MySQL|dbserver1|snapshot  	gtid_mode                                     = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,923 INFO   MySQL|dbserver1|snapshot  	gtid_next                                     = AUTOMATIC                                       [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,924 INFO   MySQL|dbserver1|snapshot  	gtid_owned                                    =                                                 [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,924 INFO   MySQL|dbserver1|snapshot  	gtid_purged                                   =                                                 [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,925 INFO   MySQL|dbserver1|snapshot  	innodb_api_enable_binlog                      = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,925 INFO   MySQL|dbserver1|snapshot  	innodb_locks_unsafe_for_binlog                = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,926 INFO   MySQL|dbserver1|snapshot  	innodb_version                                = 5.7.14                                          [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,927 INFO   MySQL|dbserver1|snapshot  	log_statements_unsafe_for_binlog              = ON                                              [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,927 INFO   MySQL|dbserver1|snapshot  	max_binlog_cache_size                         = 18446744073709547520                            [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,927 INFO   MySQL|dbserver1|snapshot  	max_binlog_size                               = 1073741824                                      [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,927 INFO   MySQL|dbserver1|snapshot  	max_binlog_stmt_cache_size                    = 18446744073709547520                            [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,928 INFO   MySQL|dbserver1|snapshot  	protocol_version                              = 10                                              [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,928 INFO   MySQL|dbserver1|snapshot  	session_track_gtids                           = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,928 INFO   MySQL|dbserver1|snapshot  	slave_type_conversions                        =                                                 [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,929 INFO   MySQL|dbserver1|snapshot  	sync_binlog                                   = 1                                               [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,930 INFO   MySQL|dbserver1|snapshot  	tls_version                                   = TLSv1,TLSv1.1                                   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,932 INFO   MySQL|dbserver1|snapshot  	tx_isolation                                  = REPEATABLE-READ                                 [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,937 INFO   MySQL|dbserver1|snapshot  	tx_read_only                                  = OFF                                             [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,937 INFO   MySQL|dbserver1|snapshot  	version                                       = 5.7.14-log                                      [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,938 INFO   MySQL|dbserver1|snapshot  	version_comment                               = MySQL Community Server (GPL)                    [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,938 INFO   MySQL|dbserver1|snapshot  	version_compile_machine                       = x86_64                                          [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,939 INFO   MySQL|dbserver1|snapshot  	version_compile_os                            = Linux                                           [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,939 INFO   MySQL|dbserver1|snapshot  Step 0: disabling autocommit and enabling repeatable read transactions   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,941 INFO   MySQL|dbserver1|snapshot  Step 1: start transaction with consistent snapshot   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,943 INFO   MySQL|dbserver1|snapshot  Step 2: flush and obtain global read lock (preventing writes to database)   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,944 INFO   MySQL|dbserver1|snapshot  Step 3: read binlog position of MySQL master   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,946 INFO   MySQL|dbserver1|snapshot  	 using binlog 'mysql-bin.000003' at position '154' and gtid ''   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,946 INFO   MySQL|dbserver1|snapshot  Step 4: read list of available databases   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,948 INFO   MySQL|dbserver1|snapshot  	 list of available databases is: [information_schema, inventory, mysql, performance_schema, sys]   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,949 INFO   MySQL|dbserver1|snapshot  Step 5: read list of available tables in each database   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,952 INFO   MySQL|dbserver1|snapshot  	 'information_schema.CHARACTER_SETS' is filtered out, discarding   [io.debezium.connector.mysql.SnapshotReader]
...
2016-08-16 01:25:19,988 INFO   MySQL|dbserver1|snapshot  	 'information_schema.INNODB_SYS_TABLESTATS' is filtered out, discarding   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,990 INFO   MySQL|dbserver1|snapshot  	 including 'inventory.customers'   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,990 INFO   MySQL|dbserver1|snapshot  	 including 'inventory.orders'   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,991 INFO   MySQL|dbserver1|snapshot  	 including 'inventory.products'   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,991 INFO   MySQL|dbserver1|snapshot  	 including 'inventory.products_on_hand'   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:19,993 INFO   MySQL|dbserver1|snapshot  	 'mysql.columns_priv' is filtered out, discarding   [io.debezium.connector.mysql.SnapshotReader]
...
2016-08-16 01:25:20,133 INFO   MySQL|dbserver1|snapshot  	 'sys.x$waits_global_by_latency' is filtered out, discarding   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,134 INFO   MySQL|dbserver1|snapshot  Step 6: generating DROP and CREATE statements to reflect current database schemas:   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,158 INFO   MySQL|dbserver1|snapshot  	DROP TABLE IF EXISTS inventory.products_on_hand   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,190 INFO   MySQL|dbserver1|snapshot  	DROP TABLE IF EXISTS inventory.customers   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,193 INFO   MySQL|dbserver1|snapshot  	DROP TABLE IF EXISTS inventory.orders   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,195 INFO   MySQL|dbserver1|snapshot  	DROP TABLE IF EXISTS inventory.products   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,202 INFO   MySQL|dbserver1|snapshot  	DROP DATABASE IF EXISTS inventory   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,206 INFO   MySQL|dbserver1|snapshot  	CREATE DATABASE inventory   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,209 INFO   MySQL|dbserver1|snapshot  	USE inventory   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,223 INFO   MySQL|dbserver1|snapshot  	CREATE TABLE `customers` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `first_name` varchar(255) NOT NULL,
  `last_name` varchar(255) NOT NULL,
  `email` varchar(255) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `email` (`email`)
) ENGINE=InnoDB AUTO_INCREMENT=1005 DEFAULT CHARSET=latin1   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,250 INFO   MySQL|dbserver1|snapshot  	CREATE TABLE `orders` (
  `order_number` int(11) NOT NULL AUTO_INCREMENT,
  `order_date` date NOT NULL,
  `purchaser` int(11) NOT NULL,
  `quantity` int(11) NOT NULL,
  `product_id` int(11) NOT NULL,
  PRIMARY KEY (`order_number`),
  KEY `order_customer` (`purchaser`),
  KEY `ordered_product` (`product_id`),
  CONSTRAINT `orders_ibfk_1` FOREIGN KEY (`purchaser`) REFERENCES `customers` (`id`),
  CONSTRAINT `orders_ibfk_2` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=10005 DEFAULT CHARSET=latin1   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,265 INFO   MySQL|dbserver1|snapshot  	CREATE TABLE `products` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL,
  `description` varchar(512) DEFAULT NULL,
  `weight` float DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=110 DEFAULT CHARSET=latin1   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,271 INFO   MySQL|dbserver1|snapshot  	CREATE TABLE `products_on_hand` (
  `product_id` int(11) NOT NULL,
  `quantity` int(11) NOT NULL,
  PRIMARY KEY (`product_id`),
  CONSTRAINT `products_on_hand_ibfk_1` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,283 INFO   MySQL|dbserver1|snapshot  Step 7: releasing global read lock to enable MySQL writes   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,285 INFO   MySQL|dbserver1|snapshot  Step 7: blocked writes to MySQL for a total of 00:00:00.341   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,287 INFO   MySQL|dbserver1|snapshot  Step 8: scanning contents of 4 tables   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,289 INFO   MySQL|dbserver1|snapshot  Step 8: - scanning table 'inventory.customers' (1 of 4 tables)   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,293 INFO   MySQL|dbserver1|snapshot  Step 8: - 4 of 4 rows scanned from table 'inventory.customers' after 00:00:00.004   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,294 INFO   MySQL|dbserver1|snapshot  Step 8: - scanning table 'inventory.orders' (2 of 4 tables)   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,302 INFO   MySQL|dbserver1|snapshot  Step 8: - 4 of 4 rows scanned from table 'inventory.orders' after 00:00:00.008   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,304 INFO   MySQL|dbserver1|snapshot  Step 8: - scanning table 'inventory.products' (3 of 4 tables)   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,306 INFO   MySQL|dbserver1|snapshot  Step 8: - 9 of 9 rows scanned from table 'inventory.products' after 00:00:00.001   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,306 INFO   MySQL|dbserver1|snapshot  Step 8: - scanning table 'inventory.products_on_hand' (4 of 4 tables)   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,308 INFO   MySQL|dbserver1|snapshot  Step 8: - 9 of 9 rows scanned from table 'inventory.products_on_hand' after 00:00:00.002   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,309 INFO   MySQL|dbserver1|snapshot  Step 8: scanned 26 rows in 4 tables in 00:00:00.021   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,309 INFO   MySQL|dbserver1|snapshot  Step 9: committing transaction   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,311 INFO   MySQL|dbserver1|snapshot  Completed snapshot in 00:00:00.713   [io.debezium.connector.mysql.SnapshotReader]
2016-08-16 01:25:20,620 WARN   ||  Error while fetching metadata with correlation id 0 : {dbserver1=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2016-08-16 01:25:20,855 WARN   ||  Error while fetching metadata with correlation id 3 : {dbserver1.inventory.customers=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2016-08-16 01:25:21,076 WARN   ||  Error while fetching metadata with correlation id 8 : {dbserver1.inventory.orders=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2016-08-16 01:25:21,310 WARN   ||  Error while fetching metadata with correlation id 11 : {dbserver1.inventory.products=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2016-08-16 01:25:21,548 WARN   ||  Error while fetching metadata with correlation id 15 : {dbserver1.inventory.products_on_hand=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2016-08-16 01:25:21,754 INFO   MySQL|dbserver1|binlog  Connected to MySQL binlog at 192.168.99.100:3306, starting at binlog file 'mysql-bin.000003', pos=154, row=0   [io.debezium.connector.mysql.BinlogReader]
2016-08-16 01:26:19,201 INFO   ||  Finished WorkerSourceTask{id=inventory-connector-0} commitOffsets successfully in 10 ms   [org.apache.kafka.connect.runtime.WorkerSourceTask]
...
----

Let's look into this output in more detail. First, Debezium log output makes use of _mapped diagnostic contexts_, or MDC, which allow the log messages to include thread-specific information like the connector type (e.g., `MySQL` in the above log messages after "INFO" or "WARN" fields), the logical name of the connector (e.g., `dbserver1` above), and the connector's activity (e.g., `snapshot` and `binlog`). Hopefully these will make it easier to understand what is going on in the multi-threaded Kafka Connect service.

Now, if we look at these log statements, we can see that the connector starts, describes information about the MySQL server and the user the connector is using, performs a consistent snapshot with 9 steps, and then starts reading the binlog at the same point where the snapshot was taken. Since our `inventory` database is quite small, the snapshot process goes quite quickly: 0.713 seconds as shown in one of the log messages above, and actually much of that was spent writing out the many log lines that make it easy to track the progress of the connector. This will take longer with larger databases, but the log messages do describe which of the 9 steps are performed with a global read lock on the MySQL server. (See the link:/docs/connectors/mysql[MySQL connector documentation] for more details.)

[NOTE]
====
Debezium 0.3 uses result set streaming during the snapshot process to page through the results. This makes it possible to snapshot very large databases without memory problems.
====

After the snapshot completes, the MySQL connector will generally output very little information using `INFO` or `WARN` level messages. 

There's one more thing in these log messages to mention. The five warning log messages near the end of the sample output above sound ominous, but are basically telling us that new Kafka topics were created and Kafka had to assign a new leader. Note the names of the topics:

* `dbserver1.inventory.products`
* `dbserver1.inventory.products_on_hand`
* `dbserver1.inventory.customers`
* `dbserver1.inventory.orders`

As described in the link:/docs/connectors/mysql/#topic-names[MySQL connector documentation], each topic names start with `dbserver1`, which is the logical name we gave our connector. Each topic name also includes `inventory`, which is the name of the database. Finally, each topic name concludes with the name of one of the tables in the `inventory` database. In other words, all of the data change events describing rows in the each table appear in separate topics.

Let's look at all of the data change events in the `dbserver1.inventory.customers` topic. Again, we'll use the `debezium/kafka` Docker image to start a new container that connects to Kafka to watch the topic from the beginning of the topic:

[source,bash,indent=0]
----
    $ docker run -it --name watcher --rm --link zookeeper:zookeeper debezium/kafka:0.3 watch-topic -a -k dbserver1.inventory.customers
----

Again, we use the `--rm` flag since we want the container to be removed when it stops, and we use the `-a` flag on `watch-topic` to signal that we want to see _all_ events since the beginning of the topic. (If we were to remove the `-a` flag, we'd see only the events that are recorded in the topic _after_ we start watching.) The `-k` flag specifies that the output should include the event's key, which in our case contains the row's primary key. Here's the output:

[source,bash,indent=0]
----
    ...
Contents of topic dbserver1.inventory.customers:
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1001}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":null,"after":{"id":1001,"first_name":"Sally","last_name":"Thomas","email":"sally.thomas@acme.com"},"source":{"name":"dbserver1","server_id":0,"ts_sec":0,"gtid":null,"file":"mysql-bin.000003","pos":154,"row":0,"snapshot":true},"op":"c","ts_ms":1471310719597}}
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1002}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":null,"after":{"id":1002,"first_name":"George","last_name":"Bailey","email":"gbailey@foobar.com"},"source":{"name":"dbserver1","server_id":0,"ts_sec":0,"gtid":null,"file":"mysql-bin.000003","pos":154,"row":0,"snapshot":true},"op":"c","ts_ms":1471310719597}}
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1003}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":null,"after":{"id":1003,"first_name":"Edward","last_name":"Walker","email":"ed@walker.com"},"source":{"name":"dbserver1","server_id":0,"ts_sec":0,"gtid":null,"file":"mysql-bin.000003","pos":154,"row":0,"snapshot":true},"op":"c","ts_ms":1471310719597}}
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1004}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":null,"after":{"id":1004,"first_name":"Anne","last_name":"Kretchmar","email":"annek@noanswer.org"},"source":{"name":"dbserver1","server_id":0,"ts_sec":0,"gtid":null,"file":"mysql-bin.000003","pos":154,"row":0,"snapshot":true},"op":"c","ts_ms":1471310719597}}
----

[NOTE]
====
This utility keeps watching, so any new events would automatically appear as long as the utility keeps running. And this `watch-topic` utility is very simple and is limited in functionality and usefulness - we use it here simply to get an understanding of the kind of events that our connector generates. Applications that want to consume events would instead use Kafka consumers, and those consumer libraries offer far more flexibility and power. In fact, properly configured clients enable our applications to never miss any events, even when those applications crash or shutdown gracefullly.
====

These events happen to be encoded in JSON, since that's how we configured our Kafka Connect service. Each event includes one JSON document for the key, and one for the value. Let's look at the last event in more detail, by first reformatting the event's _key_ to be easier to read:

[source,json,indent=0]
----
  {
    "schema": {
      "type": "struct",
      "name": "dbserver1.inventory.customers.Key"
      "optional": false,
      "fields": [
        {
          "field": "id",
          "type": "int32",
          "optional": false
        }
      ]
    },
    "payload": {
      "id": 1004
    }
  }
----

The event's key has two parts: a `schema` and `payload`. The `schema` contains a Kafka Connect schema describing what is in the payload, and in our case that means that the `payload` is a struct named `dbserver1.inventory.customers.Key` that is not optional and has one required field named `id` of type `int32`.

If we look at the value of the key's `payload` field, we'll see that it is indeed a structure (which in JSON is just an object) with a single `id` field, whose value is `1004`.

Therefore, we interpret this event as applying to the row in the `inventory.customers` table (output from the connector named `dbserver1`) whose `id` primary key column had a value of `1004`.

Now let's look at the same event's _value_, which again we reformat to be easier to read:

[source,json,indent=0]
----
{
  "schema": {
    "name": "dbserver1.inventory.customers.Envelope",
    "version": 1,
    "optional": false,
    "type": "struct",
    "fields": [
      {
        "field": "before",
        "name": "dbserver1.inventory.customers.Value",
        "optional": true,
        "type": "struct",
        "fields": [
          {
            "type": "int32",
            "optional": false,
            "field": "id"
          },
          {
            "type": "string",
            "optional": false,
            "field": "first_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "last_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "email"
          }
        ]
      },
      {
        "field": "after",
        "name": "dbserver1.inventory.customers.Value",
        "optional": true,
        "type": "struct",
        "fields": [
          {
            "type": "int32",
            "optional": false,
            "field": "id"
          },
          {
            "type": "string",
            "optional": false,
            "field": "first_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "last_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "email"
          }
        ]
      },
      {
        "field": "source",
        "name": "io.debezium.connector.mysql.Source",
        "optional": false,
        "type": "struct",
        "fields": [
          {
            "type": "string",
            "optional": false,
            "field": "name"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "server_id"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "ts_sec"
          },
          {
            "type": "string",
            "optional": true,
            "field": "gtid"
          },
          {
            "type": "string",
            "optional": false,
            "field": "file"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "pos"
          },
          {
            "type": "int32",
            "optional": false,
            "field": "row"
          },
          {
            "type": "boolean",
            "optional": true,
            "field": "snapshot"
          }
        ]
      },
      {
        "type": "string",
        "optional": false,
        "field": "op"
      },
      {
        "type": "int64",
        "optional": true,
        "field": "ts_ms"
      }
    ]
  },
  "payload": {
    "before": null,
    "after": {
      "id": 1004,
      "first_name": "Anne",
      "last_name": "Kretchmar",
      "email": "annek@noanswer.org"
    },
    "source": {
      "name": "dbserver1",
      "server_id": 0,
      "ts_sec": 0,
      "gtid": null,
      "file": "mysql-bin.000003",
      "pos": 154,
      "row": 0,
      "snapshot": true
    },
    "op": "c",
    "ts_ms": 1471310719597
  }
}
----

This portion of the event is much larger, but like the event's _key_ this, too, has a `schema` and a `payload`. The `schema` contains a Kafka Connect schema named `dbserver1.inventory.customers.Envelope` (version 1) that can contain 5 fields:

* `op` is a mandatory field that contains a string value describing the type of operation. Values for the MySQL connector are `c` for create (or insert), `u` for update, `d` for delete, and `r` for read (in the case of a non-initial snapshot).
* `before` is an optional field that if present contains the state of the row _before_ the event occurred. The structure will  be described by the `dbserver1.inventory.customers.Value` Kafka Connect schema, which the `dbserver1` connector uses for all rows in the `inventory.customers` table.
* `after` is an optional field that if present contains the state of the row _after_ the event occurred. The structure is describe by the same `dbserver1.inventory.customers.Value` Kafka Connect schema used in `before`.
* `source` is a mandatory field that conains a structure describing the source metadata for the event, which in the case of MySQL contains several fields: the connector name, the name of the binlog file where the event was recorded, the position in that binlog file where the event appeared, the row within the event (if there is more than one), whether this event was part of a snapshot, and if available the MySQL server ID, and the timestamp in seconds.
* `ts_ms` is optional and if present contains the time (using the system clock in the JVM running the Kafka Connect task) at which the connector processed the event. 

If we look at the `payload` of the event's _value_, we can see the information in the event, namely that it is describing that the row was created, contains the `id`, `first_name`, `last_name`, and `email` of the inserted row.

[TIP]
====
You may have noticed that the JSON representations of the events are much larger than the rows they describe. This is because Kafka Connect ships with every event key and value the _schema_ that describes the _payload_. Over time, this structure may change, and having the schemas for the key and value in the event itself makes it much easier for consuming applications to understand the messages, especially as they evolve over time. 

The Debezium MySQL connector constructs these schemas based upon the structure of the database tables. If you use DDL statements to alter the table definitions in the MySQL databases, the connector reads these DDL statements and updates its Kafka Connect schemas. This is the only way that each event is structured exactly like the table from where it originated at the time the event occurred. But the Kafka topic containing all of the events for a single table might have events that correspond to each state of the table definition.

The JSON converter does produce very verbose events since it includes the key and value schemas in every message. The link:http://docs.confluent.io/3.0.0/schema-registry/docs/index.html[Avro converter], on the other hand, is far smarter and results in far smaller event messages. The Avro converter transforms each Kafka Connect schema into an Avro schema and stores the Avro schemas in a separate Schema Registry service. Thus when the Avro converter serializes an event message, it places only an unique identifier for the schema along with an Avro-encoded binary representation of the value. Thus, the serialized messages transferred over the wire and stored in Kafka are far smaller than they appear above. In fact, the Avro Converter is able to use Avro schema evolution techniques to maintain the history of each schema in the Schema Registry.
====

We can compare these to the state of the database. Go back to the terminal that is running the MySQL command line client, and run the following statement:

[source,sql,indent=0]
----
    mysql> SELECT * FROM customers;
----

which produces the following output:

[source,sql,indent=0]
----
    +------+------------+-----------+-----------------------+
    | id   | first_name | last_name | email                 |
    +------+------------+-----------+-----------------------+
    | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
    | 1002 | George     | Bailey    | gbailey@foobar.com    |
    | 1003 | Edward     | Walker    | ed@walker.com         |
    | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
    +------+------------+-----------+-----------------------+
    4 rows in set (0.00 sec)
----

As we can see, all of our event records match the database. 

Now that we're monitoring changes, what happens when we *change* one of the records in the database? Run the following statement in the MySQL command line client:

[source,sql,indent=0]
----
    mysql> UPDATE customers SET first_name='Anne Marie' WHERE id=1004;
----

which produces the following output:

[source,indent=0]
----
    Query OK, 1 row affected (0.05 sec)
    Rows matched: 1  Changed: 1  Warnings: 0
----

Rerun the `select ...` statement to see the updated table:

[source,sql,indent=0]
----
    mysql> select * from customers;
    +------+------------+-----------+-----------------------+
    | id   | first_name | last_name | email                 |
    +------+------------+-----------+-----------------------+
    | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
    | 1002 | George     | Bailey    | gbailey@foobar.com    |
    | 1003 | Edward     | Walker    | ed@walker.com         |
    | 1004 | Anne Marie | Kretchmar | annek@noanswer.org    |
    +------+------------+-----------+-----------------------+
    4 rows in set (0.00 sec)
----

Now, go back to the terminal running `watch-topic` and we should see a _new_ fifth event:

[source,json,indent=0]
----
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1004}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":{"id":1004,"first_name":"Anne","last_name":"Kretchmar","email":"annek@noanswer.org"},"after":{"id":1004,"first_name":"Anne Marie","last_name":"Kretchmar","email":"annek@noanswer.org"},"source":{"name":"dbserver1","server_id":223344,"ts_sec":1471311,"gtid":null,"file":"mysql-bin.000003","pos":364,"row":0,"snapshot":true},"op":"u","ts_ms":1471311575246}}
----

Let's reformat the new event's _key_ to be easier to read:

[source,json,indent=0]
----
  {
    "schema": {
      "type": "struct",
      "name": "dbserver1.inventory.customers.Key"
      "optional": false,
      "fields": [
        {
          "field": "id",
          "type": "int32",
          "optional": false
        }
      ]
    },
    "payload": {
      "id": 1004
    }
  }
----

This key is exactly the same key as what we saw in the fourth record. Here's that new event's _value_ formatted to be easier to read:

[source,json,indent=0]
----
{
  "schema": {
    "name": "dbserver1.inventory.customers.Envelope",
    "type": "struct",
    "optional": false,
    "version": 1,
    "fields": [
      {
        "field": "before",
        "name": "dbserver1.inventory.customers.Value",
        "optional": true,
        "type": "struct",
        "fields": [
          {
            "type": "int32",
            "optional": false,
            "field": "id"
          },
          {
            "type": "string",
            "optional": false,
            "field": "first_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "last_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "email"
          }
        ]
      },
      {
        "field": "after",
        "name": "dbserver1.inventory.customers.Value",
        "optional": true,
        "type": "struct",
        "fields": [
          {
            "type": "int32",
            "optional": false,
            "field": "id"
          },
          {
            "type": "string",
            "optional": false,
            "field": "first_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "last_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "email"
          }
        ]
      },
      {
        "field": "source",
        "name": "io.debezium.connector.mysql.Source",
        "optional": false,
        "type": "struct",
        "fields": [
          {
            "type": "string",
            "optional": false,
            "field": "name"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "server_id"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "ts_sec"
          },
          {
            "type": "string",
            "optional": true,
            "field": "gtid"
          },
          {
            "type": "string",
            "optional": false,
            "field": "file"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "pos"
          },
          {
            "type": "int32",
            "optional": false,
            "field": "row"
          },
          {
            "type": "boolean",
            "optional": true,
            "field": "snapshot"
          }
        ]
      },
      {
        "type": "string",
        "optional": false,
        "field": "op"
      },
      {
        "type": "int64",
        "optional": true,
        "field": "ts_ms"
      }
    ]
  },
  "payload": {
    "before": {
      "id": 1004,
      "first_name": "Anne",
      "last_name": "Kretchmar",
      "email": "annek@noanswer.org"
    },
    "after": {
      "id": 1004,
      "first_name": "Anne Marie",
      "last_name": "Kretchmar",
      "email": "annek@noanswer.org"
    },
    "source": {
      "name": "dbserver1",
      "server_id": 223344,
      "ts_sec": 1471311,
      "gtid": null,
      "file": "mysql-bin.000003",
      "pos": 364,
      "row": 0,
      "snapshot": true
    },
    "op": "u",
    "ts_ms": 1471311575246
  }
}
----

When we compare this to the value in the fourth event, we see no changes in the `schema` section and a couple of changes in the `payload` section:

* The `op` field value is now `u`, signifying that this row changed because of an update
* The `before` field now has the state of the row with the values before the database commit
* The `after` field now has the updated state of the row, and here was can see that the `first_name` value is now `Anne Marie`.
* The `source` field structure has many of the same values as before, except the `ts_sec` and `pos` fields have changed (and the `file` might have changed in other circumstances).
* The `ts_ms` shows the timestamp that Debezium processed this event.

There are several things we can learn by just looking at this `payload` section. We can compare the `before` and `after` structures to determine what actually changed in this row because of the commit. The `source` structure tells us information about MySQL's record of this change (providing traceability), but more importantly this has information we can compare to other events in this and other topics to know whether this event occurred before, after, or as part of the same MySQL commit as other events.

So far we've seen samples of _create_ and _update_ events. Now, let's look at _delete_ events. Since Anne Marie has not placed any orders, we can remove her record from our database using the MySQL command line client:

[source,sql,indent=0]
----
    mysql> DELETE FROM customers WHERE id=1004;
----

In our terminal running `watch-topic`, we see _two_ new events:

[source,json,indent=0]
----
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1004}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":{"id":1004,"first_name":"Anne Marie","last_name":"Kretchmar","email":"annek@noanswer.org"},"after":null,"source":{"name":"dbserver1","server_id":223344,"ts_sec":1471311,"gtid":null,"file":"mysql-bin.000003","pos":725,"row":0,"snapshot":true},"op":"d","ts_ms":1471311790968}}
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1004}}	{"schema":null,"payload":null}
----

What happened? We only deleted one row, but we now have two events. To understand what the MySQL connector does, let's look at the first of our two new messages. Here's the _key_ reformatted to be easier to read:

[source,json,indent=0]
----
  {
    "schema": {
      "type": "struct",
      "name": "dbserver1.inventory.customers.Key"
      "optional": false,
      "fields": [
        {
          "field": "id",
          "type": "int32",
          "optional": false
        }
      ]
    },
    "payload": {
      "id": 1004
    }
  }
----

Once again, this key is exactly the same key as in the previous two events we looked at. Here's the _value_ of the first new event, formatted to be easier to read:

[source,json,indent=0]
----
{
    "schema": {...},
    "payload": {
      "before": {
        "id": 1004,
        "first_name": "Anne Marie",
        "last_name": "Kretchmar",
        "email": "annek@noanswer.org"
      },
      "after": null,
      "source": {
        "name": "mysql-server-1",
        "server_id": 223344,
        "ts_sec": 1465581,
        "gtid": null,
        "file": "mysql-bin.000003",
        "pos": 805,
        "row": 0,
        "snapshot": null
      },
      "op": "d",
      "ts_ms": 1465581902461
    }
----

Again, the `schema` is identical to the previous messages, but the `payload` fragment has a few things of note:

* The `op` field value is now `d`, signifying that this row was deleted
* The `before` field now has the state of the row that was deleted with the database commit
* The `after` field is null, signifying that the row no longer exists
* The `source` field structure has many of the same values as before, except the `ts_sec` and `pos` fields have changed (and the `file` might have changed in other circumstances).
* The `ts_ms` shows the timestamp that Debezium processed this event.

This event gives a consumer all kinds of information that it can use to process the removal of this row. We include the old values because some consumers might require them in order to properly handle the removal, and without it they may have to resort to far more complex behavior.

Remember that we saw two events when we deleted the row? Let's look at that second event. Here's the _key_ for the event:

[source,json,indent=0]
----
  {
    "schema": {
      "type": "struct",
      "name": "dbserver1.inventory.customers.Key"
      "optional": false,
      "fields": [
        {
          "field": "id",
          "type": "int32",
          "optional": false
        }
      ]
    },
    "payload": {
      "id": 1004
    }
  }
----

Once again, this key is exactly the same key as in the previous three events we looked at. Here's the _value_ of that same event:

[source,json,indent=0]
----
{
  "schema": null,
  "payload": null
}
----

What gives? Well, all of the Kafka topics that the MySQL connector writes to can be set up to be _log compacted_, which means that Kafka can remove older messages from the topic as long as there is at least one message later in the topic with the exact same key. This is Kafka's way to collect the garbage. This last event is what Debezium calls a _tombstone_ event, and because it has a key and an empty value Kafka understands it can remove all prior messages with this same key.

Kafka log compaction is great, because it still allows consumers to read the topic from the very beginning and not miss any events.


[[restart-kafka-connect]]
=== Restart the Kafka Connect service

One feature of the Kafka Connect service is that it automatically manages tasks for the registered connectors. And, because it stores its data in Kafka, if a running service stops or goes away completely, upon restart (perhaps on another host) the server will start any non-running tasks. To demostrate this, let's stop our Kafka Connect service, change some data in the database, and restart our service. 

In a new terminal, use the following Docker commands to stop the `connect` container that is running our Kafka Connect service:

[source,bash,indent=0]
----
    $ docker stop connect
----

Stopping the container like this stops the process running inside of it, but the Kafka Connect service handles this by gracefully shutting down. And because we ran the container with the `--rm` flag, Docker removed the container after it stopped it.

While the service is down, let's go back to the MySQL command line client and add a few records:

[source,sql,indent=0]
----
    mysql> INSERT INTO customers VALUES (default, "Sarah", "Thompson", "kitt@acme.com");
    mysql> INSERT INTO customers VALUES (default, "Kenneth", "Anderson", "kander@acme.com");
----

Notice that in the terminal where we're running `watch-topic`, there's been no update. Also, we're still able to watch the topic because Kafka is still running. 

[TIP]
====
In a production system, you would have enough brokers to handle the producers and consumers, and to maintain a minimum number of in sync replicas for each topic. So if enough brokers fail such that there are not the minimum number of ISRs, Kafka should become unavailable. Producers, like the Debezium connectors, and consumers will simply wait patiently for the Kafka cluster or network to recover. Yes, that means that your consumers might temporarily see no change events as data is changed in the databases, but that's because none are being produced. As soon as the Kafka cluster is restarted or the network recovers, Debezium will continue producing change events and your consumers will continue consuming events where they left off.
====

Now, in a new terminal, start a new container using the _same_ command we used before:

[source,bash,indent=0]
----
    $ docker run -it --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my-connect-offsets --link zookeeper:zookeeper --link kafka:kafka debezium/connect:0.3
----

This creates a whole new container that runs the Kafka Connect distributed service, and since we've intialized it with the same topic information, the new service connects to Kafka, read the previous service's configuration, and starts the registered connectors that will continue exactly where they last left off.

Here's the last few lines from this restarted service:

[source,bash,indent=0]
----
...
2016-08-16 02:50:06,437 INFO   MySQL|dbserver1|task  Found existing offset: {ts_sec=1471315, file=mysql-bin.000003, pos=438, row=0, server_id=223344}   [io.debezium.connector.mysql.MySqlConnectorTask]
...
2016-08-16 02:50:06,477 INFO   MySQL|dbserver1|task  Kafka version : 0.10.0.1   [org.apache.kafka.common.utils.AppInfoParser]
2016-08-16 02:50:06,477 INFO   MySQL|dbserver1|task  Kafka commitId : a7a17cdec9eaa6c5   [org.apache.kafka.common.utils.AppInfoParser]
2016-08-16 02:50:06,581 INFO   MySQL|dbserver1|task  Discovered coordinator 172.17.0.3:9092 (id: 2147483646 rack: null) for group 3ad7c83a-da2f-4ede-b9c6-64c058ed3ba6.   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 02:50:06,583 INFO   MySQL|dbserver1|task  Revoking previously assigned partitions [] for group 3ad7c83a-da2f-4ede-b9c6-64c058ed3ba6   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2016-08-16 02:50:06,583 INFO   MySQL|dbserver1|task  (Re-)joining group 3ad7c83a-da2f-4ede-b9c6-64c058ed3ba6   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 02:50:06,593 INFO   MySQL|dbserver1|task  Successfully joined group 3ad7c83a-da2f-4ede-b9c6-64c058ed3ba6 with generation 1   [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]
2016-08-16 02:50:06,594 INFO   MySQL|dbserver1|task  Setting newly assigned partitions [dbhistory.inventory-0] for group 3ad7c83a-da2f-4ede-b9c6-64c058ed3ba6   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2016-08-16 02:50:07,105 INFO   MySQL|dbserver1|task  Step 0: Get all known binlogs from MySQL   [io.debezium.connector.mysql.MySqlConnectorTask]
...
2016-08-16 02:50:07,643 INFO   MySQL|dbserver1|binlog  Connected to MySQL binlog at 192.168.99.100:3306, starting at binlog file 'mysql-bin.000003', pos=438, row=0   [io.debezium.connector.mysql.BinlogReader]
2016-08-16 02:50:07,643 INFO   ||  Source task WorkerSourceTask{id=inventory-connector-0} finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSourceTask]
----

As you can see, these lines show that the service finds the offsets previously recorded by the last task before it was shut down, and that it then connects to the MySQL database, starts reading the binlog from that position, and generates events from any changes in the MySQL database since that point in time.

Jump back to the terminal running `watch-topic`, and you should now see events for our two new records:

[source,json,indent=0]
----
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1005}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":null,"after":{"id":1005,"first_name":"Sarah","last_name":"Thompson","email":"kitt@acme.com"},"source":{"name":"dbserver1","server_id":223344,"ts_sec":1471314,"gtid":null,"file":"mysql-bin.000003","pos":1046,"row":0,"snapshot":null},"op":"c","ts_ms":1471314885877}}
{"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"dbserver1.inventory.customers.Key"},"payload":{"id":1006}}	{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"before"},{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":true,"name":"dbserver1.inventory.customers.Value","field":"after"},{"type":"struct","fields":[{"type":"string","optional":false,"field":"name"},{"type":"int64","optional":false,"field":"server_id"},{"type":"int64","optional":false,"field":"ts_sec"},{"type":"string","optional":true,"field":"gtid"},{"type":"string","optional":false,"field":"file"},{"type":"int64","optional":false,"field":"pos"},{"type":"int32","optional":false,"field":"row"},{"type":"boolean","optional":true,"field":"snapshot"}],"optional":false,"name":"io.debezium.connector.mysql.Source","field":"source"},{"type":"string","optional":false,"field":"op"},{"type":"int64","optional":true,"field":"ts_ms"}],"optional":false,"name":"dbserver1.inventory.customers.Envelope","version":1},"payload":{"before":null,"after":{"id":1006,"first_name":"Kenneth","last_name":"Anderson","email":"kander@acme.com"},"source":{"name":"dbserver1","server_id":223344,"ts_sec":1471314,"gtid":null,"file":"mysql-bin.000003","pos":1356,"row":0,"snapshot":null},"op":"c","ts_ms":1471314885878}}
----

These events are _create_ events that are similar to what we saw before. The important point to understand, though, is that Debezium will still report all of the changes in a database even when it is not running, as long as it is restarted before the MySQL database starts purging those commits we missed from its binlog.


[[exploration]]
=== Exploration

Go ahead and use the MySQL command line client to add, modify, and remove rows to the database tables, and see the effect on the topics. You may need to start multiple `watch-topic` commands for each topic. And remember that you can't remove a row that is referenced by a foreign key. Have fun!

[[cleanup]]
=== Clean up

You can use Docker to stop all of the running containers:

[source,bash,indent=0]
----
    $ docker stop mysqlterm watcher connect mysql kafka zookeeper
----

Again, since we used the `--rm` flag when starting the connectors, Docker should remove them right after it stops them. We can verify that all of the other processes are stopped and removed:

[source,bash,indent=0]
----
    $ docker ps -a
----

Of course, if any are still running, simply stop them using `docker stop <name>` or `docker stop <containerId>`.



