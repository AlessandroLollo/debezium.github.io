= API/Schema registry for Debezium
jpechane
:awestruct-tags: [ schema, avro, apicurio ]
:awestruct-layout: blog-post

Messages streamed from a database by Debezium are (in developer parlance) strong-typed.
This means that consumer processing the messages should be aware of the types of data stored in the message.
This problem can be solved in multiple ways:

. the message structure is passed out-of-band to the consumer who is able to process the data stored in it
. the message contains metadata called schema that is embedded in the message
. the message contains a reference to a registry that contains the associated metadata

The example of the first case is `JsonConverter`.
`JsonConverter` can operate in two modes - with and without schemas.
When configured to work without schemas it generates a plain `JSON` message where the consumer either needs to know the types of each field beforehand or it needs to execute heuristic rules to "guess" and map values to datatypes.
While this approach is quite flexible it can fail for example in case of mixed string/number values in a single field.
Also, constraints associated with the types are usually lost.

An example of such a message is:

[source,json]
----
{
  "before": null,
  "after": {
    "id": 1001,
    "first_name": "Sally",
    "last_name": "Thomas",
    "email": "sally.thomas@acme.com"
  },
  "source": {
    "version": "1.1.0.Final",
    "connector": "mysql",
    "name": "dbserver1",
    "ts_ms": 0,
    "snapshot": "true",
    "db": "inventory",
    "table": "customers",
    "server_id": 0,
    "gtid": null,
    "file": "mysql-bin.000003",
    "pos": 154,
    "row": 0,
    "thread": null,
    "query": null
  },
  "op": "c",
  "ts_ms": 1586331101491,
  "transaction": null
}
----

The example of the second case is again `JsonConverter`.
If we keep the schemas enable the `JSON` message will consist of two parts - `schema` and `payload`.
The `payload` is exactly the same as in the previous case but the `schema` contains description of the message, its fields, field types and associated type constraints.
This enables the consumer to process the message in type safe way.
The drawback of this approach is that the size of the message has increased significantly as the schema is quite large object and it is copied over and over for each message even if it is unchanged.

The example of a message with a schema clearly shows that the schema itself can be significantly larger than the payload and is not economical to use.

[source,json]
----
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "struct",
        "fields": [
          {
            "type": "int32",
            "optional": false,
            "field": "id"
          },
          {
            "type": "string",
            "optional": false,
            "field": "first_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "last_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "email"
          }
        ],
        "optional": true,
        "name": "dbserver1.inventory.customers.Value",
        "field": "before"
      },
      {
        "type": "struct",
        "fields": [
          {
            "type": "int32",
            "optional": false,
            "field": "id"
          },
          {
            "type": "string",
            "optional": false,
            "field": "first_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "last_name"
          },
          {
            "type": "string",
            "optional": false,
            "field": "email"
          }
        ],
        "optional": true,
        "name": "dbserver1.inventory.customers.Value",
        "field": "after"
      },
      {
        "type": "struct",
        "fields": [
          {
            "type": "string",
            "optional": false,
            "field": "version"
          },
          {
            "type": "string",
            "optional": false,
            "field": "connector"
          },
          {
            "type": "string",
            "optional": false,
            "field": "name"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "ts_ms"
          },
          {
            "type": "string",
            "optional": true,
            "name": "io.debezium.data.Enum",
            "version": 1,
            "parameters": {
              "allowed": "true,last,false"
            },
            "default": "false",
            "field": "snapshot"
          },
          {
            "type": "string",
            "optional": false,
            "field": "db"
          },
          {
            "type": "string",
            "optional": true,
            "field": "table"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "server_id"
          },
          {
            "type": "string",
            "optional": true,
            "field": "gtid"
          },
          {
            "type": "string",
            "optional": false,
            "field": "file"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "pos"
          },
          {
            "type": "int32",
            "optional": false,
            "field": "row"
          },
          {
            "type": "int64",
            "optional": true,
            "field": "thread"
          },
          {
            "type": "string",
            "optional": true,
            "field": "query"
          }
        ],
        "optional": false,
        "name": "io.debezium.connector.mysql.Source",
        "field": "source"
      },
      {
        "type": "string",
        "optional": false,
        "field": "op"
      },
      {
        "type": "int64",
        "optional": true,
        "field": "ts_ms"
      },
      {
        "type": "struct",
        "fields": [
          {
            "type": "string",
            "optional": false,
            "field": "id"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "total_order"
          },
          {
            "type": "int64",
            "optional": false,
            "field": "data_collection_order"
          }
        ],
        "optional": true,
        "field": "transaction"
      }
    ],
    "optional": false,
    "name": "dbserver1.inventory.customers.Envelope"
  },
  "payload": {
    "before": null,
    "after": {
      "id": 1001,
      "first_name": "Sally",
      "last_name": "Thomas",
      "email": "sally.thomas@acme.com"
    },
    "source": {
      "version": "1.1.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 0,
      "snapshot": "true",
      "db": "inventory",
      "table": "customers",
      "server_id": 0,
      "gtid": null,
      "file": "mysql-bin.000003",
      "pos": 154,
      "row": 0,
      "thread": null,
      "query": null
    },
    "op": "c",
    "ts_ms": 1586331101491,
    "transaction": null
  }
}
----

== Registry

Then there is the third approach that combines strong points of the first two while it removes their drawbacsk at the cost of introducing a new component - registry - that stores and versions message schemas.

Debezium's preferred choice is https://github.com/Apicurio/apicurio-registry[Apicurio Registry]. The project provides not only the registry itself but also client libraries and tight integration with Apache Kafka and Kafka Connect in form of serializers and converters.

Apicurio enables Debezium and consumers to exchange messages whose schema is stored into the registry and pass only reference to the schema in the message.
Also as the source table and thus message schema evolves, the registry versions the schemas so not only current but also historical schemas are available.

Apicurio provides multiple serialization formats out-of-the-box:

* https://avro.apache.org/[Avro]
* JSON with externalized schema support
* https://developers.google.com/protocol-buffers[Protocol Buffers]

Every serializer and deserializer knows how to automatically interact with Apicurio API so the consumer is isolated from it as an implementation detail.
The only information necessary is the location of the registry.

Apicurio also provides API compatibility layers for Confluent Schema Registry and IBM Schema Registry.
This is a very useful feature as it enables the use of 3rd party tools like https://github.com/edenhill/kafkacat[kafkacat] even if they are not yet aware of the native API.

=== JSON Converter

There is a https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql-apicurio.yaml[Docker Compose] based deployment example that deploys Apicurio Registry side-by-side with standard Debezium example setup.

To follow the example you need to clone the Debezium https://github.com/debezium/debezium-examples/[example repository].

[source,bash]
----
$ cd tutorial
$ export DEBZIUM_VERSION=1.1

# Start the deployment
$ docker-compose -f docker-compose-mysql-apicurio.yaml up -d --build

# Start the connector
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @register-mysql-apicurio-converter-json.json

# Read content of the first message
$ docker run --rm --tty   --network tutorial_default debezium/tooling bash -c 'kafkacat -b kafka:9092 -C -o beginning -q -t dbserver1.inventory.customers -c 1 | jq .'
----

The resulting message should look like:

[source,json]
----
{
  "schemaId": 48,
  "payload": {
    "before": null,
    "after": {
      "id": 1001,
      "first_name": "Sally",
      "last_name": "Thomas",
      "email": "sally.thomas@acme.com"
    },
    "source": {
      "version": "1.1.0.Final",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 0,
      "snapshot": "true",
      "db": "inventory",
      "table": "customers",
      "server_id": 0,
      "gtid": null,
      "file": "mysql-bin.000003",
      "pos": 154,
      "row": 0,
      "thread": null,
      "query": null
    },
    "op": "c",
    "ts_ms": 1586334283147,
    "transaction": null
  }
}
----

The JSON message contains the full payload and at the same time a reference to a schema with id `48`.
It is possible to query the schema from the registry either using `id` or using a schema symbolic name as defined by Debezium documentation.
In this case both commands:

[source,bash]
----
$ docker run --rm --tty   --network tutorial_default debezium/tooling bash -c 'http http://apicurio:8080/ids/64 | jq .'
$ docker run --rm --tty   --network tutorial_default debezium/tooling bash -c 'http http://apicurio:8080/artifacts/dbserver1.inventory.customers-value | jq .'
----

results in the same schema description:

[source,json]
----
{
  "type": "struct",
  "fields": [
    {
      "type": "struct",
      "fields": [
        {
          "type": "int32",
          "optional": false,
          "field": "id"
        },
        {
          "type": "string",
          "optional": false,
          "field": "first_name"
        },
        {
          "type": "string",
          "optional": false,
          "field": "last_name"
        },
        {
          "type": "string",
          "optional": false,
          "field": "email"
        }
      ],
      "optional": true,
      "name": "dbserver1.inventory.customers.Value",
      "field": "before"
    },
...
  ],
  "optional": false,
  "name": "dbserver1.inventory.customers.Envelope"
}
----

which is the same as we have seen in the `JSON` with schema example.

The connector registration request differs in a few lines from the standard one:

[source,json]
----
        (1) "key.converter": "io.apicurio.registry.utils.converter.ExtJsonConverter",
        (2) "key.converter.apicurio.registry.url": "http://apicurio:8080",
        (3) "key.converter.apicurio.registry.global-id": "io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy",
        (1) "value.converter": "io.apicurio.registry.utils.converter.ExtJsonConverter",
        (2) "value.converter.apicurio.registry.url": "http://apicurio:8080",
        (3) "value.converter.apicurio.registry.global-id": "io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy"
----

. Apicurio JSON converter is used as both key and value converter
. Option points to the actual Apicurio registry endpoint
. This setting ensures that it is posible to automatically register the schema id which is typical setting in Debezium deployment

=== Avro Converter

So far we have demonstrated serialization of message into the `JSON` format only.
While `JSON` format has a lot of advantages like easy human readibility it still contains a lot of content unrelated to the data itself.

To transfer really only the data without any significant overhead it is useful to use binary format serialization like Avro format.
In this case, we would pack the data only without any field names and other ceremony and again the message will contain a reference to a schema stored in the registry.

Let's look at how easily the Avro serialization can be used with Apicurio's Avro converter.

[source,bash]
----
# Tear down the previous deployment
$ docker-compose -f docker-compose-mysql-apicurio.yaml down

# Start the deployment
$ docker-compose -f docker-compose-mysql-apicurio.yaml up -d --build

# Start the connector
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @register-mysql-apicurio-converter-avro.json

# Read content of the first message
$ docker run --rm --tty   --network tutorial_default debezium/tooling bash -c 'kafkacat -b kafka:9092 -C -o beginning -q -t dbserver1.inventory.customers -c 1 -s avro -r http://apicurio:8080/ccompat | jq .'
----

We introduced options `-s avro` and `-r http://apicurio:8080/ccompat` to declare that we are using Avro format and location of the Apicurio registry endpoint.
Please note that the endpoint points not to the native API but to the compatibility layer so even if `kafkacat` is not yet aware of the new API it can easily interoperate with Apicurio.

The resulting message should look like:

[source,json]
----
{
  "before": null,
  "after": {
    "Value": {
      "id": 1001,
      "first_name": "Sally",
      "last_name": "Thomas",
      "email": "sally.thomas@acme.com"
    }
  },
  "source": {
    "version": "1.1.0.Final",
    "connector": "mysql",
    "name": "dbserver1",
    "ts_ms": 0,
    "snapshot": {
      "string": "true"
    },
    "db": "inventory",
    "table": {
      "string": "customers"
    },
    "server_id": 0,
    "gtid": null,
    "file": "mysql-bin.000003",
    "pos": 154,
    "row": 0,
    "thread": null,
    "query": null
  },
  "op": "c",
  "ts_ms": {
    "long": 1586336163386
  },
  "transaction": null
}
----

In this case we get only message payload without the schema identifier but we can still query the registry using schema name:

[source,bash]
----
$ docker run --rm --tty   --network tutorial_default debezium/tooling bash -c 'http http://apicurio:8080/artifacts/dbserver1.inventory.customers-value | jq .'
----

The resulting schema description is slightly different for the previous ones as it has an Avro flavour:

[source,json]
----
{
  "type": "record",
  "name": "Envelope",
  "namespace": "dbserver1.inventory.customers",
  "fields": [
    {
      "name": "before",
      "type": [
        "null",
        {
          "type": "record",
          "name": "Value",
          "fields": [
            {
              "name": "id",
              "type": "int"
            },
            {
              "name": "first_name",
              "type": "string"
            },
            {
              "name": "last_name",
              "type": "string"
            },
            {
              "name": "email",
              "type": "string"
            }
          ],
          "connect.name": "dbserver1.inventory.customers.Value"
        }
      ],
      "default": null
    },
    {
      "name": "after",
      "type": [
        "null",
        "Value"
      ],
      "default": null
    },
...
  ],
  "connect.name": "dbserver1.inventory.customers.Envelope"
}
----

The connector registration request also differs from the standard one in a handful of lines

[source,json]
----
        (1) "key.converter": "io.apicurio.registry.utils.converter.AvroConverter",
        (2) "key.converter.apicurio.registry.url": "http://apicurio:8080",
        (3) "key.converter.apicurio.registry.converter.serializer": "io.apicurio.registry.utils.serde.AvroKafkaSerializer",
        (3) "key.converter.apicurio.registry.converter.deserializer": "io.apicurio.registry.utils.serde.AvroKafkaDeserializer",
        (4) "key.converter.apicurio.registry.global-id": "io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy",
        (5) "key.converter.apicurio.registry.id-handler": "io.apicurio.registry.utils.serde.strategy.ConfluentIdHandler",
        (1) "value.converter": "io.apicurio.registry.utils.converter.AvroConverter",
        (2) "value.converter.apicurio.registry.url": "http://apicurio:8080",
        (3) "value.converter.apicurio.registry.converter.serializer": "io.apicurio.registry.utils.serde.AvroKafkaSerializer",
        (3) "value.converter.apicurio.registry.converter.deserializer": "io.apicurio.registry.utils.serde.AvroKafkaDeserializer",
        (4) "value.converter.apicurio.registry.global-id": "io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy",
        (5) "value.converter.apicurio.registry.id-handler": "io.apicurio.registry.utils.serde.strategy.ConfluentIdHandler"
----

. Apicurio Avro converter is used as both key and value converter
. Option points to the actual Apicurio registry endpoint
. Prescribe which serializer and deserializer should be used by the converter
. This setting ensures that it is posible to automatically register the schema id which is typical setting in Debezium deployment
. This option is necessary only if we want to ensure binary compatibility with non-Apicurio aware tools like `kafkacat`

== Conclusion

In this article we discussed mutliple approaches to message/schema association.
The Apicurio registry was presented as a solution for schema sotrage and versioning and we have demonstarted how Apicurio can be integrated with Debezium connectors to efficiently deliver messages with schema to the consumer.


== About Debezium

Debezium is an open-source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of http://kafka.apache.org/[Kafka] and provides http://kafka.apache.org/documentation.html#connect[Kafka Connect] compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is link:/license/[open source] under the http://www.apache.org/licenses/LICENSE-2.0.html[Apache License, Version 2.0].

== Get involved

We hope you find Debezium interesting and useful and want to give it a try.
Follow us on Twitter https://twitter.com/debezium[@debezium], https://gitter.im/debezium/user[chat with us on Gitter],
or join our https://groups.google.com/forum/#!forum/debezium[mailing list] to talk with the community.
All of the code is open-source https://github.com/debezium/[on GitHub],
so build the code locally and help us improve our existing connectors and add even more connectors.
If you find problems or have an idea on how we can improve Debezium, please let us know or https://issues.redhat.com/projects/DBZ/issues/[log an issue].
